# -*- coding: utf-8 -*-
"""hw2_complete

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j1tGc_JX5IzKmRa0jGiH3TphiTwaavTI
"""

import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, DepthwiseConv2D, SeparableConv2D, Input, add, Dropout
import matplotlib.pyplot as plt

def build_model1():
    model = Sequential([
        Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D((4, 4), strides=(4, 4)),
        Flatten(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dense(10, activation='softmax')
    ])
    return model

def build_model2():
    model = Sequential([
        Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)),
        BatchNormalization(),
        SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        SeparableConv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'),
        BatchNormalization(),
        SeparableConv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        SeparableConv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        SeparableConv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D((4, 4), strides=(4, 4)),
        Flatten(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dense(10, activation='softmax')
    ])
    return model

from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Concatenate
from tensorflow.keras.models import Model

def build_model3():
    input_layer = Input(shape=(32, 32, 3))

    # First convolutional layer with dropout
    conv1 = Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(input_layer)
    conv1 = BatchNormalization()(conv1)
    conv1 = Dropout(0.2)(conv1)

    # Second convolutional layer with dropout
    conv2 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(conv1)
    conv2 = BatchNormalization()(conv2)
    conv2 = Dropout(0.2)(conv2)

    # Concatenate feature maps from conv1 and conv2
    skip1 = Concatenate()([conv1, conv2])

    # Third convolutional layer with dropout
    conv3 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(skip1)
    conv3 = BatchNormalization()(conv3)
    conv3 = Dropout(0.2)(conv3)

    # Fourth convolutional layer with dropout
    conv4 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(conv3)
    conv4 = BatchNormalization()(conv4)
    conv4 = Dropout(0.2)(conv4)

    # Concatenate feature maps from skip1 and conv4
    skip2 = Concatenate()([skip1, conv4])

    # Max pooling
    max_pool = MaxPooling2D((4, 4), strides=(4, 4))(skip2)

    # Flatten and dense layers
    flatten = Flatten()(max_pool)
    dense1 = Dense(128, activation='relu')(flatten)
    dense1 = BatchNormalization()(dense1)
    dense1 = Dropout(0.5)(dense1)
    output = Dense(10, activation='softmax')(dense1)

    model = Model(inputs=input_layer, outputs=output)
    return model

if __name__ == '__main__':
    # Load CIFAR-10 dataset and split into training, validation, and testing sets
    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()
    train_images, val_images = train_images[:45000], train_images[45000:]
    train_labels, val_labels = train_labels[:45000], train_labels[45000:]
    train_images, val_images, test_images = train_images / 255.0, val_images / 255.0, test_images / 255.0

    # Build and compile model1
    model1 = build_model1()
    model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model1.summary()

    # Train model1
    model1.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    # Evaluate model1 on test set
    test_loss, test_accuracy = model1.evaluate(test_images, test_labels)
    print("Test accuracy for model1:", test_accuracy)

    # Build and compile model2
    model2 = build_model2()
    model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model2.summary()

    # Train model2
    model2.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    # Evaluate model2 on test set
    test_loss, test_accuracy = model2.evaluate(test_images, test_labels)
    print("Test accuracy for model2:", test_accuracy)

    # Build and compile model3
    model3 = build_model3()
    model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model3.summary()

    # Train model3
    model3.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    # Evaluate model3 on test set
    test_loss, test_accuracy = model3.evaluate(test_images, test_labels)
    print("Test accuracy for model3:", test_accuracy)

    # Save model3
    model3.save("best_model.h5")

    # Load and preprocess test image
    test_image_path = "test_image_classname.jpg"
    test_image = tf.keras.preprocessing.image.load_img(test_image_path, target_size=(32, 32))
    test_image_array = tf.keras.preprocessing.image.img_to_array(test_image)
    test_image_array = tf.expand_dims(test_image_array, axis=0)
    test_image_array = test_image_array / 255.0

    # Visualize the test image
    plt.imshow(test_image_array[0])
    plt.axis('off')
    plt.show()

    # Predict class label
    class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']
    predictions = model1.predict(test_image_array)
    predicted_class_index = tf.argmax(predictions[0]).numpy()
    predicted_class = class_names[predicted_class_index]
    print("Predicted class for model1:", predicted_class)